# -*- coding: utf-8 -*-
"""Data651_Jorge Armando Barrera Ceballos_109161343.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ELDetPpUX4o20lwAZTpNbR69jrnVdRk-
"""

# Jorge Armando Barrera Ceballos
# UID: 109161343
# Data 651: Big Data Analytics
# Final Project code

#Save the data to a Directory inside Databricks

dbutils.fs.cp("/FileStore/tables/dataset_amazon-4.csv", "/Test/dataset_amazon-4.csv")

#Install libraries

pip install koalas nltk

#Import data using Koalas

import databricks.koalas as ks

data = ks.read_csv('/Test/dataset_amazon-4.csv',sep = '|')

#Code to drop missing data from the data set
#Code to create dichotomus variable with positive or negative labels
#Code to compute number of reviews per label

#Checking the number of rows of the data base and deleting rows with missing values
rows = len(data.axes[0])
#Quick check
print(rows)

#Deleting rows with missing values
data = data.dropna()

#Checking the number of rows of the data base after deleting rows with missing values
rows = len(data.axes[0])
print(rows)

#Change the data type of review/score to integer
data.dtypes
data["overall"] = ks.to_numeric(data["overall"])
data.dtypes

#Classify the review/score in positive and negative (4 and 5 = positive)
data['labels'] = data['overall']
data['labels'] = data['overall'].apply(lambda x : 1 if x > 3 else 0)
#Quick check
data.dtypes
data.head(5)

#Count the positive and negative cases in labels
print(data.groupby('labels')['reviewText'].count())

#Calculate the number of positive and negative cases in the dataframe
print('Percentage of negative reviews %.1f %%' % ((data.groupby('labels')['reviewText'].count()[0])/len(data)*100.0))
print('Percentage of positive reviews %.1f %%' % ((data.groupby('labels')['reviewText'].count()[1])/len(data)*100.0))

#Separate the data frame in positive and negative reviews based on the score
neg = data.loc[data['labels']==0]
pos = data.loc[data['labels']==1]
print('Size of the positive data:', pos.shape)
print('Size of the negative data:', neg.shape)

#Install nltk library
#pip install nltk

#Install libraries

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string

#Download packages from nltk

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

#Print stopwords from nltk

print(stopwords.words('english'))

#Functions for preprocessing reviews

#Create variables to make lemmatizacion, stop words removal and translation

lemmatizer = nltk.WordNetLemmatizer()
stop_words = stopwords.words('english')
translation = str.maketrans(string.punctuation,' '*len(string.punctuation))

#Function to translate, lower and tokenize reviews

def preprocessing0(line):
    filtered=[]
    line = line.translate(translation)
    line = line.lower()
    line = word_tokenize(line)
    for t in line:
        if t not in stop_words:
            filtered.append(t)
    return ' '.join(filtered)

#Function to lemmatize reviews

def preprocessing(line):
    tokens=[]
    line = word_tokenize(line)
    for t in line:
        lemmatized = lemmatizer.lemmatize(t)
        tokens.append(lemmatized)
    return ' '.join(tokens)

#Preprocessing reviews

#Apply first function preprocessing0 to reviews

import pandas as pd
import numpy as np

pos_array = pos['reviewText'].to_numpy()
neg_array = neg['reviewText'].to_numpy()

pos_data = []
neg_data = []
for p in pos_array:
    pos_data.append(preprocessing0(p))

for n in neg_array:
    neg_data.append(preprocessing0(n))

#Concatenate positive and negative reviews

data = pos_data + neg_data

#Preprocessing reviews

#Apply second function preprocessing to reviews

pos_data1 = []
neg_data1 = []
for p in pos_data:
    pos_data1.append(preprocessing(p))

for n in neg_data:
    neg_data1.append(preprocessing(n))

#Concatenate positive and negative reviews

data1 = pos_data1 + neg_data1

#Print some records to check them

print(data1[1000:1005])

#Create a list of positive and negative labels and concatenate them

pos_array_score = pos['labels'].to_numpy()
neg_array_score = neg['labels'].to_numpy()

pos_score = []
neg_score = []
for p in pos_array_score:
    pos_score.append(p)

for n in neg_array_score:
    neg_score.append(n)

#Concatenate positive and negative reviews

labels = pos_score + neg_score

#Quick check
print(pos_score[0:5])
print(neg_score[0:10])
print(labels[0:5])

#SPlit data into train and test data sets stratifying by labels
from sklearn.model_selection import train_test_split

[Data_train,Data_test,Train_labels,Test_labels] = train_test_split(data1,labels , test_size=0.3, random_state=42,stratify=labels)

#Quick check
print(Data_train[0:5])

#Logistic regression model with most important features
#Best Model

#Import libraries
from nltk.classify.scikitlearn import SklearnClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.naive_bayes import BernoulliNB
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.linear_model import LogisticRegression

#Create pipeline
classifier = LogisticRegression(max_iter=500)
model8 = Pipeline(
    [
        ("vectorizer", TfidfVectorizer()),
        ('chi2', SelectKBest(chi2, k=1000)),
        ("classifier", classifier),
    ]
)

#Import libraries
from sklearn.model_selection import cross_validate
from sklearn.model_selection import ShuffleSplit
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import roc_auc_score

#Compute performance metrics with cross-validation, size of testing data of 30%

cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'roc_auc']
scores = cross_validate(model8, data1, labels, scoring=scoring, cv = cv)
sorted(scores.keys())

#Import libraries
import numpy as np

#Print performance metrics

print ("Logistic regression model with most important features:")
print(np.mean(scores['test_accuracy']))
print(np.mean(scores['test_precision_weighted']))
print(np.mean(scores['test_recall_weighted']))
print(np.mean(scores['test_roc_auc']))

#Check pipeline

model8.fit(Data_train, Train_labels)

# Get the names of each feature

feature_names0 = model8.named_steps["vectorizer"].get_feature_names_out()
feature_names = model8.named_steps["chi2"].get_feature_names_out(feature_names0)

# Get the coefficients of each feature

coefs = model8.named_steps["classifier"].coef_.flatten()

# Sort features by importance

import pandas as pd

# Zip coefficients and names together and make a DataFrame
zipped = zip(feature_names, coefs)
df = pd.DataFrame(zipped, columns=["feature", "value"])

# Sort the features by the absolute value of their coefficient

df["abs_value"] = df["value"].apply(lambda x: abs(x))
df["colors"] = df["value"].apply(lambda x: "green" if x > 0 else "red")
print(df.head(10))
df = df.sort_values("abs_value", ascending=False)

# Print 10 most important features

print(df.head(10))
print(df.shape)

#Plot 20 most important features

import seaborn as sns
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1, 1, figsize=(12, 7))
sns.barplot(x="feature",
            y="value",
            data=df.head(20),
           palette=df.head(20)["colors"])
ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=20)
ax.set_title("Top 20 Features", fontsize=25)
ax.set_ylabel("Coef", fontsize=22)
ax.set_xlabel("Feature Name", fontsize=22)

#Logistic regression model with dimensionality reduction

#Create pipeline
classifier = LogisticRegression(max_iter=500)
model9 = Pipeline(
    [
        ("vectorizer", TfidfVectorizer()),
        ('chi2', SelectKBest(chi2, k=1000)),
        ('svd', TruncatedSVD(n_components=200)),
        ("classifier", classifier),
    ]
)

#Compute performance metrics with cross-validation, size of testing data of 30%

cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'roc_auc']
scores = cross_validate(model9, data1, labels, scoring=scoring, cv = cv)
sorted(scores.keys())

#Print performance metrics

print ("Logistic regression model with most important features:")
print(np.mean(scores['test_accuracy']))
print(np.mean(scores['test_precision_weighted']))
print(np.mean(scores['test_recall_weighted']))
print(np.mean(scores['test_roc_auc']))

#Logistic regression model with 2-gram

#Create pipeline
classifier = LogisticRegression(max_iter=500)
model10 = Pipeline(
    [
        ("vectorizer", TfidfVectorizer(analyzer = 'word', ngram_range=(2,2))),
        ('chi2', SelectKBest(chi2, k=1000)),
        ("classifier", classifier)
    ]
)

#Compute performance metrics with cross-validation, size of testing data of 30%

cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'roc_auc']
scores = cross_validate(model10, data1, labels, scoring=scoring, cv = cv)
sorted(scores.keys())

#Print performance metrics

print ("Logistic regression model with most important features:")
print(np.mean(scores['test_accuracy']))
print(np.mean(scores['test_precision_weighted']))
print(np.mean(scores['test_recall_weighted']))
print(np.mean(scores['test_roc_auc']))

#Logistic regression model with 3-gram

#Create pipeline
classifier = LogisticRegression(max_iter=500)
model11 = Pipeline(
    [
        ("vectorizer", TfidfVectorizer(analyzer = 'word', ngram_range=(3,3))),
        ('chi2', SelectKBest(chi2, k=1000)),
        ("classifier", classifier)
    ]
)

#Compute performance metrics with cross-validation, size of testing data of 30%

cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'roc_auc']
scores = cross_validate(model11, data1, labels, scoring=scoring, cv = cv)
sorted(scores.keys())

#Print performance metrics

print ("Logistic regression model with most important features:")
print(np.mean(scores['test_accuracy']))
print(np.mean(scores['test_precision_weighted']))
print(np.mean(scores['test_recall_weighted']))
print(np.mean(scores['test_roc_auc']))

#Naive Bayes model with most important features

#Create pipeline
classifier = BernoulliNB()
model12 = Pipeline(
    [
        ("vectorizer", TfidfVectorizer()),
        ('chi2', SelectKBest(chi2, k=1000)),
        ("classifier", classifier),
    ]
)

#Compute performance metrics with cross-validation, size of testing data of 30%

cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'roc_auc']
scores = cross_validate(model12, data1, labels, scoring=scoring, cv = cv)
sorted(scores.keys())

#Print performance metrics

print ("Logistic regression model with most important features:")
print(np.mean(scores['test_accuracy']))
print(np.mean(scores['test_precision_weighted']))
print(np.mean(scores['test_recall_weighted']))
print(np.mean(scores['test_roc_auc']))

#Naive Bayes model with dimensionality reduction

#Create pipeline
classifier = BernoulliNB()
model14 = Pipeline(
    [
        ("vectorizer", TfidfVectorizer()),
        ('chi2', SelectKBest(chi2, k=1000)),
        ('svd', TruncatedSVD(n_components=200)),
        ("classifier", classifier),
    ]
)

#Compute performance metrics with cross-validation, size of testing data of 30%

cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'roc_auc']
scores = cross_validate(model14, data1, labels, scoring=scoring, cv = cv)
sorted(scores.keys())

#Print performance metrics

print ("Logistic regression model with most important features:")
print(np.mean(scores['test_accuracy']))
print(np.mean(scores['test_precision_weighted']))
print(np.mean(scores['test_recall_weighted']))
print(np.mean(scores['test_roc_auc']))

#Naive Bayes model with 2-gram

#Create pipeline
classifier = BernoulliNB()
model15 = Pipeline(
    [
        ("vectorizer", TfidfVectorizer(analyzer = 'word', ngram_range=(2,2))),
        ('chi2', SelectKBest(chi2, k=1000)),
        ("classifier", classifier)
    ]
)

#Compute performance metrics with cross-validation, size of testing data of 30%

cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'roc_auc']
scores = cross_validate(model15, data1, labels, scoring=scoring, cv = cv)
sorted(scores.keys())

#Print performance metrics

print ("Logistic regression model with most important features:")
print(np.mean(scores['test_accuracy']))
print(np.mean(scores['test_precision_weighted']))
print(np.mean(scores['test_recall_weighted']))
print(np.mean(scores['test_roc_auc']))

#Naives Bayes model with 3-gram

#Create pipeline
classifier = BernoulliNB()
model16 = Pipeline(
    [
        ("vectorizer", TfidfVectorizer(analyzer = 'word', ngram_range=(3,3))),
        ('chi2', SelectKBest(chi2, k=1000)),
        ("classifier", classifier)
    ]
)

#Compute performance metrics with cross-validation, size of testing data of 30%

cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'roc_auc']
scores = cross_validate(model16, data1, labels, scoring=scoring, cv = cv)
sorted(scores.keys())

#Print performance metrics

print ("Logistic regression model with most important features:")
print(np.mean(scores['test_accuracy']))
print(np.mean(scores['test_precision_weighted']))
print(np.mean(scores['test_recall_weighted']))
print(np.mean(scores['test_roc_auc']))